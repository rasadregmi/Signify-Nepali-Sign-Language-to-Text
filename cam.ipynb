{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('nslt_model.h5')\n",
    "\n",
    "# Load class names in Nepali\n",
    "class_names = [\"नमस्कार\", \"म\", \"घर\", \"धन्यवाद\"]\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a Nepali-compatible font (Download and place in your working directory)\n",
    "fontpath = \"NotoSansDevanagari-Regular.ttf\"\n",
    "font = ImageFont.truetype(fontpath, 30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip frame horizontally for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Process frame with MediaPipe Hands\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get hand coordinates\n",
    "            x = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y = [lm.y for lm in hand_landmarks.landmark]\n",
    "            \n",
    "            # Convert normalized coordinates to pixel values\n",
    "            height, width = frame.shape[:2]\n",
    "            x_min = int(min(x) * width)\n",
    "            x_max = int(max(x) * width)\n",
    "            y_min = int(min(y) * height)\n",
    "            y_max = int(max(y) * height)\n",
    "            \n",
    "            # Expand bounding box\n",
    "            expand = 0.1\n",
    "            x_min = max(0, x_min - int((x_max - x_min) * expand))\n",
    "            x_max = min(width, x_max + int((x_max - x_min) * expand))\n",
    "            y_min = max(0, y_min - int((y_max - y_min) * expand))\n",
    "            y_max = min(height, y_max + int((y_max - y_min) * expand))\n",
    "            \n",
    "            # Crop and preprocess hand image\n",
    "            hand_image = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_image.size != 0:\n",
    "                hand_image = cv2.resize(hand_image, (224, 224))\n",
    "                hand_image = np.expand_dims(hand_image, axis=0) / 255.0\n",
    "                \n",
    "                # Make prediction\n",
    "                pred = model.predict(hand_image)\n",
    "                pred_class = class_names[np.argmax(pred)]\n",
    "                confidence = np.max(pred)\n",
    "                \n",
    "                # Convert OpenCV image to PIL for Nepali text rendering\n",
    "                pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "                # Draw bounding box\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline=\"green\", width=2)\n",
    "\n",
    "                # Draw text using Nepali font\n",
    "                text = f'{pred_class} ({confidence:.2f})'\n",
    "                draw.text((x_min + 10, y_min - 40), text, font=font, fill=(0, 255, 0))\n",
    "\n",
    "                # Convert back to OpenCV format\n",
    "                frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Sign Language Detection', frame)\n",
    "\n",
    "    # Exit on 'q' press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('DSML3model.h5')\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "# Class mapping\n",
    "CLASS_MAP = {\n",
    "    0: 'क', 1: 'ख', 2: 'ग', 3: 'घ', 4: 'ङ', 5: 'च',\n",
    "    6: 'छ', 7: 'ज', 8: 'झ', 9: 'ञ', 10: 'ट', 11: 'ठ',\n",
    "    12: 'ड', 13: 'ढ', 14: 'ण', 15: 'त', 16: 'थ', 17: 'द',\n",
    "    18: 'ध', 19: 'न', 20: 'प', 21: 'फ', 22: 'ब', 23: 'भ',\n",
    "    24: 'म', 25: 'य', 26: 'र', 27: 'ल', 28: 'व', 29: 'श',\n",
    "    30: 'ष', 31: 'स', 32: 'ह', 33: 'क्ष', 34: 'त्र', 35: 'ज्ञ'\n",
    "}\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a Nepali-compatible font\n",
    "fontpath = \"NotoSansDevanagari-Regular.ttf\"  # Ensure the font file is in the working directory\n",
    "font = ImageFont.truetype(fontpath, 30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Mirror the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Get bounding box coordinates\n",
    "        x_coords = [lm.x * frame.shape[1] for lm in hand_landmarks.landmark]\n",
    "        y_coords = [lm.y * frame.shape[0] for lm in hand_landmarks.landmark]\n",
    "        \n",
    "        x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "        y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "        \n",
    "        # Add padding\n",
    "        padding = int(0.2 * max(x_max - x_min, y_max - y_min))\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(frame.shape[1], x_max + padding)\n",
    "        y_max = min(frame.shape[0], y_max + padding)\n",
    "        \n",
    "        # Crop and preprocess hand region\n",
    "        hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "        if hand_roi.size > 0:\n",
    "            processed = cv2.resize(hand_roi, (128, 128)) / 255.0\n",
    "            prediction = model.predict(np.expand_dims(processed, 0), verbose=0)\n",
    "            pred_class = np.argmax(prediction)\n",
    "            \n",
    "            # Convert OpenCV frame to PIL format\n",
    "            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "            # Draw bounding box\n",
    "            draw.rectangle([x_min, y_min, x_max, y_max], outline=\"green\", width=2)\n",
    "\n",
    "            # Draw text using Nepali font\n",
    "            draw.text((x_min + 10, y_min - 40), CLASS_MAP[pred_class], font=font, fill=(0, 255, 0))\n",
    "\n",
    "            # Convert back to OpenCV format\n",
    "            frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Show frame\n",
    "    cv2.imshow('Nepali Sign Language Detection', frame)\n",
    "    \n",
    "    # Break on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Load the trained models\n",
    "word_model = tf.keras.models.load_model('nslt_model.h5')  # Model for common words\n",
    "alphabet_model = tf.keras.models.load_model('DSML3model.h5')  # Model for alphabets\n",
    "\n",
    "# Define class names for words\n",
    "word_class_names = [\"नमस्कार\", \"म\", \"घर\", \"धन्यवाद\"]\n",
    "\n",
    "# Define class names for Nepali alphabets\n",
    "alphabet_class_map = {\n",
    "    0: 'क', 1: 'ख', 2: 'ग', 3: 'घ', 4: 'ङ', 5: 'च',\n",
    "    6: 'छ', 7: 'ज', 8: 'झ', 9: 'ञ', 10: 'ट', 11: 'ठ',\n",
    "    12: 'ड', 13: 'ढ', 14: 'ण', 15: 'त', 16: 'थ', 17: 'द',\n",
    "    18: 'ध', 19: 'न', 20: 'प', 21: 'फ', 22: 'ब', 23: 'भ',\n",
    "    24: 'म', 25: 'य', 26: 'र', 27: 'ल', 28: 'व', 29: 'श',\n",
    "    30: 'ष', 31: 'स', 32: 'ह', 33: 'क्ष', 34: 'त्र', 35: 'ज्ञ'\n",
    "}\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a Nepali-compatible font (Ensure the font file is in the working directory)\n",
    "font_path = \"NotoSansDevanagari-Regular.ttf\"\n",
    "font = ImageFont.truetype(font_path, 30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame horizontally for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get bounding box coordinates\n",
    "            x = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            height, width = frame.shape[:2]\n",
    "            x_min, x_max = int(min(x) * width), int(max(x) * width)\n",
    "            y_min, y_max = int(min(y) * height), int(max(y) * height)\n",
    "\n",
    "            # Add padding\n",
    "            padding = int(0.2 * max(x_max - x_min, y_max - y_min))\n",
    "            x_min = max(0, x_min - padding)\n",
    "            y_min = max(0, y_min - padding)\n",
    "            x_max = min(width, x_max + padding)\n",
    "            y_max = min(height, y_max + padding)\n",
    "\n",
    "            # Crop and preprocess hand region\n",
    "            hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_roi.size > 0:\n",
    "                # Resize separately for each model\n",
    "                word_processed = cv2.resize(hand_roi, (224, 224)) / 255.0\n",
    "                alphabet_processed = cv2.resize(hand_roi, (128, 128)) / 255.0\n",
    "\n",
    "                # Expand dimensions\n",
    "                word_processed = np.expand_dims(word_processed, axis=0)\n",
    "                alphabet_processed = np.expand_dims(alphabet_processed, axis=0)\n",
    "\n",
    "                # Predict using both models\n",
    "                word_pred = word_model.predict(word_processed, verbose=0)\n",
    "                alphabet_pred = alphabet_model.predict(alphabet_processed, verbose=0)\n",
    "\n",
    "                # Select the best prediction\n",
    "                word_confidence = np.max(word_pred)\n",
    "                alphabet_confidence = np.max(alphabet_pred)\n",
    "\n",
    "                if word_confidence > alphabet_confidence:\n",
    "                    pred_class = word_class_names[np.argmax(word_pred)]\n",
    "                    confidence = word_confidence\n",
    "                else:\n",
    "                    pred_class = alphabet_class_map[np.argmax(alphabet_pred)]\n",
    "                    confidence = alphabet_confidence\n",
    "\n",
    "\n",
    "                # Convert OpenCV frame to PIL format\n",
    "                pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "                # Draw bounding box\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline=\"green\", width=2)\n",
    "\n",
    "                # Draw text using Nepali font\n",
    "                text = f'{pred_class} ({confidence:.2f})'\n",
    "                draw.text((x_min + 10, y_min - 40), text, font=font, fill=(0, 255, 0))\n",
    "\n",
    "                # Convert back to OpenCV format\n",
    "                frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Nepali Sign Language Detection', frame)\n",
    "\n",
    "    # Exit on 'q' press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
